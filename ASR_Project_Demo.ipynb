{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Installation"
      ],
      "metadata": {
        "id": "q_Ao4QAZ22cn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/hoshicasia/ASR-main.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-wkO_IcO1ydc",
        "outputId": "1467a391-a3bf-44cd-be80-3d9c88d78a08"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ASR'...\n",
            "remote: Enumerating objects: 443, done.\u001b[K\n",
            "remote: Counting objects: 100% (443/443), done.\u001b[K\n",
            "remote: Compressing objects: 100% (296/296), done.\u001b[K\n",
            "remote: Total 443 (delta 212), reused 359 (delta 128), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (443/443), 834.62 KiB | 13.91 MiB/s, done.\n",
            "Resolving deltas: 100% (212/212), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd ASR-main"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5mbxburn1nzF",
        "outputId": "c2c9fbea-671c-42c5-c519-350d9095b416"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/ASR\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 6037
        },
        "collapsed": true,
        "id": "GwxV3QSK1t2G",
        "outputId": "e85f767a-a3b3-44e9-ad73-79903794537d"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch==2.5.0 (from -r requirements.txt (line 1))\n",
            "  Using cached torch-2.5.0-cp312-cp312-manylinux1_x86_64.whl.metadata (28 kB)\n",
            "Collecting torchvision==0.20.0 (from -r requirements.txt (line 2))\n",
            "  Using cached torchvision-0.20.0-cp312-cp312-manylinux1_x86_64.whl.metadata (6.1 kB)\n",
            "Collecting torchaudio==2.5.0 (from -r requirements.txt (line 3))\n",
            "  Using cached torchaudio-2.5.0-cp312-cp312-manylinux1_x86_64.whl.metadata (6.4 kB)\n",
            "Requirement already satisfied: soundfile in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 4)) (0.13.1)\n",
            "Collecting numpy==1.26 (from -r requirements.txt (line 5))\n",
            "  Using cached numpy-1.26.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (58 kB)\n",
            "Collecting torch_audiomentations (from -r requirements.txt (line 6))\n",
            "  Using cached torch_audiomentations-0.12.0-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 7)) (4.67.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 8)) (3.10.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 9)) (2.2.2)\n",
            "Collecting comet_ml (from -r requirements.txt (line 10))\n",
            "  Using cached comet_ml-3.53.2-py3-none-any.whl.metadata (4.0 kB)\n",
            "Collecting hydra-core (from -r requirements.txt (line 11))\n",
            "  Using cached hydra_core-1.3.2-py3-none-any.whl.metadata (5.5 kB)\n",
            "Collecting torchmetrics (from -r requirements.txt (line 12))\n",
            "  Using cached torchmetrics-1.8.2-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting datasets==2.15.0 (from -r requirements.txt (line 13))\n",
            "  Using cached datasets-2.15.0-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting torchcodec==0.1.0 (from -r requirements.txt (line 14))\n",
            "  Using cached TorchCodec-0.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Collecting pyctcdecode (from -r requirements.txt (line 15))\n",
            "  Using cached pyctcdecode-0.5.0-py2.py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 16)) (0.2.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 17)) (4.57.0)\n",
            "Collecting wget (from -r requirements.txt (line 19))\n",
            "  Using cached wget-3.2-py3-none-any.whl\n",
            "Collecting black (from -r requirements.txt (line 20))\n",
            "  Using cached black-25.9.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl.metadata (83 kB)\n",
            "Collecting isort (from -r requirements.txt (line 21))\n",
            "  Using cached isort-7.0.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting pre-commit (from -r requirements.txt (line 22))\n",
            "  Using cached pre_commit-4.3.0-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch==2.5.0->-r requirements.txt (line 1)) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.5.0->-r requirements.txt (line 1)) (4.15.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch==2.5.0->-r requirements.txt (line 1)) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch==2.5.0->-r requirements.txt (line 1)) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch==2.5.0->-r requirements.txt (line 1)) (2025.3.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch==2.5.0->-r requirements.txt (line 1))\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch==2.5.0->-r requirements.txt (line 1))\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch==2.5.0->-r requirements.txt (line 1))\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch==2.5.0->-r requirements.txt (line 1))\n",
            "  Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch==2.5.0->-r requirements.txt (line 1))\n",
            "  Using cached nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch==2.5.0->-r requirements.txt (line 1))\n",
            "  Using cached nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch==2.5.0->-r requirements.txt (line 1))\n",
            "  Using cached nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch==2.5.0->-r requirements.txt (line 1))\n",
            "  Using cached nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch==2.5.0->-r requirements.txt (line 1))\n",
            "  Using cached nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.21.5 (from torch==2.5.0->-r requirements.txt (line 1))\n",
            "  Using cached nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.4.127 (from torch==2.5.0->-r requirements.txt (line 1))\n",
            "  Using cached nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch==2.5.0->-r requirements.txt (line 1))\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting triton==3.1.0 (from torch==2.5.0->-r requirements.txt (line 1))\n",
            "  Using cached triton-3.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch==2.5.0->-r requirements.txt (line 1)) (75.2.0)\n",
            "Collecting sympy==1.13.1 (from torch==2.5.0->-r requirements.txt (line 1))\n",
            "  Using cached sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision==0.20.0->-r requirements.txt (line 2)) (11.3.0)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets==2.15.0->-r requirements.txt (line 13)) (18.1.0)\n",
            "Collecting pyarrow-hotfix (from datasets==2.15.0->-r requirements.txt (line 13))\n",
            "  Using cached pyarrow_hotfix-0.7-py3-none-any.whl.metadata (3.6 kB)\n",
            "Collecting dill<0.3.8,>=0.3.0 (from datasets==2.15.0->-r requirements.txt (line 13))\n",
            "  Using cached dill-0.3.7-py3-none-any.whl.metadata (9.9 kB)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.12/dist-packages (from datasets==2.15.0->-r requirements.txt (line 13)) (2.32.4)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets==2.15.0->-r requirements.txt (line 13)) (3.6.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.12/dist-packages (from datasets==2.15.0->-r requirements.txt (line 13)) (0.70.16)\n",
            "Collecting fsspec (from torch==2.5.0->-r requirements.txt (line 1))\n",
            "  Using cached fsspec-2023.10.0-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (from datasets==2.15.0->-r requirements.txt (line 13)) (3.13.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.18.0 in /usr/local/lib/python3.12/dist-packages (from datasets==2.15.0->-r requirements.txt (line 13)) (0.35.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets==2.15.0->-r requirements.txt (line 13)) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets==2.15.0->-r requirements.txt (line 13)) (6.0.3)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy==1.13.1->torch==2.5.0->-r requirements.txt (line 1)) (1.3.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.12/dist-packages (from soundfile->-r requirements.txt (line 4)) (2.0.0)\n",
            "Collecting julius<0.3,>=0.2.3 (from torch_audiomentations->-r requirements.txt (line 6))\n",
            "  Using cached julius-0.2.7-py3-none-any.whl\n",
            "Collecting torch-pitch-shift>=1.2.2 (from torch_audiomentations->-r requirements.txt (line 6))\n",
            "  Using cached torch_pitch_shift-1.2.5-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 8)) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 8)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 8)) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 8)) (1.4.9)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 8)) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 8)) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->-r requirements.txt (line 9)) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->-r requirements.txt (line 9)) (2025.2)\n",
            "Collecting dulwich!=0.20.33,>=0.20.6 (from comet_ml->-r requirements.txt (line 10))\n",
            "  Using cached dulwich-0.24.5-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (5.4 kB)\n",
            "Collecting everett<3.2.0,>=1.0.1 (from everett[ini]<3.2.0,>=1.0.1->comet_ml->-r requirements.txt (line 10))\n",
            "  Using cached everett-3.1.0-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: jsonschema!=3.1.0,>=2.6.0 in /usr/local/lib/python3.12/dist-packages (from comet_ml->-r requirements.txt (line 10)) (4.25.1)\n",
            "Requirement already satisfied: psutil>=5.6.3 in /usr/local/lib/python3.12/dist-packages (from comet_ml->-r requirements.txt (line 10)) (5.9.5)\n",
            "Collecting python-box<7.0.0 (from comet_ml->-r requirements.txt (line 10))\n",
            "  Using cached python_box-6.1.0-py3-none-any.whl.metadata (7.8 kB)\n",
            "Requirement already satisfied: requests-toolbelt>=0.8.0 in /usr/local/lib/python3.12/dist-packages (from comet_ml->-r requirements.txt (line 10)) (1.0.0)\n",
            "Requirement already satisfied: rich>=13.3.2 in /usr/local/lib/python3.12/dist-packages (from comet_ml->-r requirements.txt (line 10)) (13.9.4)\n",
            "Requirement already satisfied: semantic-version>=2.8.0 in /usr/local/lib/python3.12/dist-packages (from comet_ml->-r requirements.txt (line 10)) (2.10.0)\n",
            "Requirement already satisfied: sentry-sdk>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from comet_ml->-r requirements.txt (line 10)) (2.40.0)\n",
            "Requirement already satisfied: simplejson in /usr/local/lib/python3.12/dist-packages (from comet_ml->-r requirements.txt (line 10)) (3.20.2)\n",
            "Requirement already satisfied: urllib3>=1.26.8 in /usr/local/lib/python3.12/dist-packages (from comet_ml->-r requirements.txt (line 10)) (2.5.0)\n",
            "Requirement already satisfied: wrapt>=1.11.2 in /usr/local/lib/python3.12/dist-packages (from comet_ml->-r requirements.txt (line 10)) (1.17.3)\n",
            "Requirement already satisfied: wurlitzer>=1.0.2 in /usr/local/lib/python3.12/dist-packages (from comet_ml->-r requirements.txt (line 10)) (3.1.1)\n",
            "Requirement already satisfied: omegaconf<2.4,>=2.2 in /usr/local/lib/python3.12/dist-packages (from hydra-core->-r requirements.txt (line 11)) (2.3.0)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.12/dist-packages (from hydra-core->-r requirements.txt (line 11)) (4.9.3)\n",
            "Collecting lightning-utilities>=0.8.0 (from torchmetrics->-r requirements.txt (line 12))\n",
            "  Using cached lightning_utilities-0.15.2-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting pygtrie<3.0,>=2.1 (from pyctcdecode->-r requirements.txt (line 15))\n",
            "  Using cached pygtrie-2.5.0-py3-none-any.whl.metadata (7.5 kB)\n",
            "Collecting hypothesis<7,>=6.14 (from pyctcdecode->-r requirements.txt (line 15))\n",
            "  Using cached hypothesis-6.142.0-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers->-r requirements.txt (line 17)) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers->-r requirements.txt (line 17)) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers->-r requirements.txt (line 17)) (0.6.2)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from black->-r requirements.txt (line 20)) (8.3.0)\n",
            "Collecting mypy-extensions>=0.4.3 (from black->-r requirements.txt (line 20))\n",
            "  Using cached mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting pathspec>=0.9.0 (from black->-r requirements.txt (line 20))\n",
            "  Using cached pathspec-0.12.1-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: platformdirs>=2 in /usr/local/lib/python3.12/dist-packages (from black->-r requirements.txt (line 20)) (4.5.0)\n",
            "Collecting pytokens>=0.1.10 (from black->-r requirements.txt (line 20))\n",
            "  Using cached pytokens-0.2.0-py3-none-any.whl.metadata (2.0 kB)\n",
            "Collecting cfgv>=2.0.0 (from pre-commit->-r requirements.txt (line 22))\n",
            "  Using cached cfgv-3.4.0-py2.py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting identify>=1.0.0 (from pre-commit->-r requirements.txt (line 22))\n",
            "  Using cached identify-2.6.15-py2.py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting nodeenv>=0.11.1 (from pre-commit->-r requirements.txt (line 22))\n",
            "  Using cached nodeenv-1.9.1-py2.py3-none-any.whl.metadata (21 kB)\n",
            "Collecting virtualenv>=20.10.0 (from pre-commit->-r requirements.txt (line 22))\n",
            "  Using cached virtualenv-20.35.3-py3-none-any.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.0->soundfile->-r requirements.txt (line 4)) (2.23)\n",
            "Collecting configobj (from everett[ini]<3.2.0,>=1.0.1->comet_ml->-r requirements.txt (line 10))\n",
            "  Using cached configobj-5.0.9-py2.py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==2.15.0->-r requirements.txt (line 13)) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==2.15.0->-r requirements.txt (line 13)) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==2.15.0->-r requirements.txt (line 13)) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==2.15.0->-r requirements.txt (line 13)) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==2.15.0->-r requirements.txt (line 13)) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==2.15.0->-r requirements.txt (line 13)) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==2.15.0->-r requirements.txt (line 13)) (1.22.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.18.0->datasets==2.15.0->-r requirements.txt (line 13)) (1.1.10)\n",
            "Requirement already satisfied: sortedcontainers<3.0.0,>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from hypothesis<7,>=6.14->pyctcdecode->-r requirements.txt (line 15)) (2.4.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema!=3.1.0,>=2.6.0->comet_ml->-r requirements.txt (line 10)) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema!=3.1.0,>=2.6.0->comet_ml->-r requirements.txt (line 10)) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema!=3.1.0,>=2.6.0->comet_ml->-r requirements.txt (line 10)) (0.27.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib->-r requirements.txt (line 8)) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->datasets==2.15.0->-r requirements.txt (line 13)) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->datasets==2.15.0->-r requirements.txt (line 13)) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->datasets==2.15.0->-r requirements.txt (line 13)) (2025.10.5)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=13.3.2->comet_ml->-r requirements.txt (line 10)) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=13.3.2->comet_ml->-r requirements.txt (line 10)) (2.19.2)\n",
            "Collecting primePy>=1.3 (from torch-pitch-shift>=1.2.2->torch_audiomentations->-r requirements.txt (line 6))\n",
            "  Using cached primePy-1.3-py3-none-any.whl.metadata (4.8 kB)\n",
            "Collecting distlib<1,>=0.3.7 (from virtualenv>=20.10.0->pre-commit->-r requirements.txt (line 22))\n",
            "  Using cached distlib-0.4.0-py2.py3-none-any.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch==2.5.0->-r requirements.txt (line 1)) (3.0.3)\n",
            "INFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting multiprocess (from datasets==2.15.0->-r requirements.txt (line 13))\n",
            "  Using cached multiprocess-0.70.18-py312-none-any.whl.metadata (7.5 kB)\n",
            "  Using cached multiprocess-0.70.17-py312-none-any.whl.metadata (7.2 kB)\n",
            "  Using cached multiprocess-0.70.15-py311-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=13.3.2->comet_ml->-r requirements.txt (line 10)) (0.1.2)\n",
            "Using cached torch-2.5.0-cp312-cp312-manylinux1_x86_64.whl (906.4 MB)\n",
            "Using cached torchvision-0.20.0-cp312-cp312-manylinux1_x86_64.whl (7.2 MB)\n",
            "Using cached torchaudio-2.5.0-cp312-cp312-manylinux1_x86_64.whl (3.4 MB)\n",
            "Using cached numpy-1.26.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.9 MB)\n",
            "Using cached datasets-2.15.0-py3-none-any.whl (521 kB)\n",
            "Using cached TorchCodec-0.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (747 kB)\n",
            "Using cached nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "Using cached nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "Using cached nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "Using cached nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "Using cached nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "Using cached nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "Using cached nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "Using cached nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "Using cached nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
            "Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Using cached nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\n",
            "Using cached sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
            "Using cached triton-3.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.6 MB)\n",
            "Using cached torch_audiomentations-0.12.0-py3-none-any.whl (48 kB)\n",
            "Using cached comet_ml-3.53.2-py3-none-any.whl (766 kB)\n",
            "Using cached hydra_core-1.3.2-py3-none-any.whl (154 kB)\n",
            "Using cached torchmetrics-1.8.2-py3-none-any.whl (983 kB)\n",
            "Using cached pyctcdecode-0.5.0-py2.py3-none-any.whl (39 kB)\n",
            "Using cached black-25.9.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl (1.7 MB)\n",
            "Using cached isort-7.0.0-py3-none-any.whl (94 kB)\n",
            "Using cached pre_commit-4.3.0-py2.py3-none-any.whl (220 kB)\n",
            "Using cached cfgv-3.4.0-py2.py3-none-any.whl (7.2 kB)\n",
            "Using cached dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "Using cached dulwich-0.24.5-cp312-cp312-manylinux_2_28_x86_64.whl (1.3 MB)\n",
            "Using cached everett-3.1.0-py2.py3-none-any.whl (35 kB)\n",
            "Using cached fsspec-2023.10.0-py3-none-any.whl (166 kB)\n",
            "Using cached hypothesis-6.142.0-py3-none-any.whl (533 kB)\n",
            "Using cached identify-2.6.15-py2.py3-none-any.whl (99 kB)\n",
            "Using cached lightning_utilities-0.15.2-py3-none-any.whl (29 kB)\n",
            "Using cached mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Using cached nodeenv-1.9.1-py2.py3-none-any.whl (22 kB)\n",
            "Using cached pathspec-0.12.1-py3-none-any.whl (31 kB)\n",
            "Using cached pygtrie-2.5.0-py3-none-any.whl (25 kB)\n",
            "Using cached python_box-6.1.0-py3-none-any.whl (27 kB)\n",
            "Using cached pytokens-0.2.0-py3-none-any.whl (12 kB)\n",
            "Using cached torch_pitch_shift-1.2.5-py3-none-any.whl (5.0 kB)\n",
            "Using cached virtualenv-20.35.3-py3-none-any.whl (6.0 MB)\n",
            "Using cached multiprocess-0.70.15-py311-none-any.whl (135 kB)\n",
            "Using cached pyarrow_hotfix-0.7-py3-none-any.whl (7.9 kB)\n",
            "Using cached distlib-0.4.0-py2.py3-none-any.whl (469 kB)\n",
            "Using cached primePy-1.3-py3-none-any.whl (4.0 kB)\n",
            "Using cached configobj-5.0.9-py2.py3-none-any.whl (35 kB)\n",
            "Installing collected packages: wget, pygtrie, primePy, everett, distlib, virtualenv, triton, torchcodec, sympy, pytokens, python-box, pyarrow-hotfix, pathspec, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, nodeenv, mypy-extensions, lightning-utilities, isort, identify, hypothesis, fsspec, dulwich, dill, configobj, cfgv, pyctcdecode, pre-commit, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, hydra-core, black, nvidia-cusolver-cu12, torch, datasets, comet_ml, torchvision, torchmetrics, torchaudio, julius, torch-pitch-shift, torch_audiomentations\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.4.0\n",
            "    Uninstalling triton-3.4.0:\n",
            "      Successfully uninstalled triton-3.4.0\n",
            "  Attempting uninstall: sympy\n",
            "    Found existing installation: sympy 1.13.3\n",
            "    Uninstalling sympy-1.13.3:\n",
            "      Successfully uninstalled sympy-1.13.3\n",
            "  Attempting uninstall: python-box\n",
            "    Found existing installation: python-box 7.3.2\n",
            "    Uninstalling python-box-7.3.2:\n",
            "      Successfully uninstalled python-box-7.3.2\n",
            "  Attempting uninstall: nvidia-nvtx-cu12\n",
            "    Found existing installation: nvidia-nvtx-cu12 12.6.77\n",
            "    Uninstalling nvidia-nvtx-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-nvtx-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.6.85\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.6.85:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.6.85\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.27.3\n",
            "    Uninstalling nvidia-nccl-cu12-2.27.3:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.27.3\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.7.77\n",
            "    Uninstalling nvidia-curand-cu12-10.3.7.77:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.7.77\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.3.0.4\n",
            "    Uninstalling nvidia-cufft-cu12-11.3.0.4:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.3.0.4\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.6.77\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.6.77\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.6.80\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.6.80:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.6.80\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.6.4.1\n",
            "    Uninstalling nvidia-cublas-cu12-12.6.4.1:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.6.4.1\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.0\n",
            "    Uninstalling fsspec-2025.3.0:\n",
            "      Successfully uninstalled fsspec-2025.3.0\n",
            "  Attempting uninstall: dill\n",
            "    Found existing installation: dill 0.3.8\n",
            "    Uninstalling dill-0.3.8:\n",
            "      Successfully uninstalled dill-0.3.8\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.4.2\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.4.2:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.4.2\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.10.2.21\n",
            "    Uninstalling nvidia-cudnn-cu12-9.10.2.21:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.10.2.21\n",
            "  Attempting uninstall: multiprocess\n",
            "    Found existing installation: multiprocess 0.70.16\n",
            "    Uninstalling multiprocess-0.70.16:\n",
            "      Successfully uninstalled multiprocess-0.70.16\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.7.1.2\n",
            "    Uninstalling nvidia-cusolver-cu12-11.7.1.2:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.7.1.2\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.8.0+cu126\n",
            "    Uninstalling torch-2.8.0+cu126:\n",
            "      Successfully uninstalled torch-2.8.0+cu126\n",
            "  Attempting uninstall: datasets\n",
            "    Found existing installation: datasets 4.0.0\n",
            "    Uninstalling datasets-4.0.0:\n",
            "      Successfully uninstalled datasets-4.0.0\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.23.0+cu126\n",
            "    Uninstalling torchvision-0.23.0+cu126:\n",
            "      Successfully uninstalled torchvision-0.23.0+cu126\n",
            "  Attempting uninstall: torchaudio\n",
            "    Found existing installation: torchaudio 2.8.0+cu126\n",
            "    Uninstalling torchaudio-2.8.0+cu126:\n",
            "      Successfully uninstalled torchaudio-2.8.0+cu126\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.0 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.0 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.0 which is incompatible.\n",
            "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2023.10.0 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed black-25.9.0 cfgv-3.4.0 comet_ml-3.53.2 configobj-5.0.9 datasets-2.15.0 dill-0.3.7 distlib-0.4.0 dulwich-0.24.5 everett-3.1.0 fsspec-2023.10.0 hydra-core-1.3.2 hypothesis-6.142.0 identify-2.6.15 isort-7.0.0 julius-0.2.7 lightning-utilities-0.15.2 multiprocess-0.70.15 mypy-extensions-1.1.0 nodeenv-1.9.1 numpy-1.26.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.4.127 pathspec-0.12.1 pre-commit-4.3.0 primePy-1.3 pyarrow-hotfix-0.7 pyctcdecode-0.5.0 pygtrie-2.5.0 python-box-6.1.0 pytokens-0.2.0 sympy-1.13.1 torch-2.5.0 torch-pitch-shift-1.2.5 torch_audiomentations-0.12.0 torchaudio-2.5.0 torchcodec-0.1.0 torchmetrics-1.8.2 torchvision-0.20.0 triton-3.1.0 virtualenv-20.35.3 wget-3.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "804ac3da243c4b34ae0bf5ff64be8e9c"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!chmod +x download_checkpoints.sh"
      ],
      "metadata": {
        "id": "5OhwhxBI5Vgd"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./download_checkpoints.sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "collapsed": true,
        "id": "Nw1FpO3_480F",
        "outputId": "d3ff44c2-ee1d-4d80-b53a-c518294232ea"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retrieving folder contents\n",
            "Retrieving folder 1K50aDUqgb5aGM7Mg2jb4W9ZBWfG1riD_ conformer_bpe_M\n",
            "Processing file 1KkuSUvlBGkiulcp8hY8jeMV3FuGOqfkd checkpoint-epoch100.pth\n",
            "Processing file 1SFtgFBMI47C1DnSAb2IRyKfoHFH7UhFz checkpoint-epoch105.pth\n",
            "Processing file 1fWyTqVw82DcGTF6hCireMrocasiZys-Z checkpoint-epoch107.pth\n",
            "Processing file 1eODvhRu72YTsXPdFQKoETtGSeaPEVnL8 checkpoint-epoch108.pth\n",
            "Processing file 1OFqS7nn3b22XQ3m433ii12bVNPSDmcpv checkpoint-epoch110.pth\n",
            "Processing file 19xo98yVcWAN7NsOw4c1lNZJr8HxG1xpU checkpoint-epoch115.pth\n",
            "Processing file 17FLeSlod_pteRme-0BhWw5ICQXzD0mKw checkpoint-epoch117.pth\n",
            "Processing file 1NhAuR9NoPd5fzBtcZYeAFZe8w_pJeUwl checkpoint-epoch120.pth\n",
            "Processing file 1Hzara3pbu93e7ugAu-bC2sLNfO_8Ro65 config.yaml\n",
            "Processing file 1BYZYuYe-LUO2HGhnd6XmFYiegsZhTjpO git_commit.txt\n",
            "Processing file 1Zs7nkNN4pQi_IP_AnvhCBC-eZh0gPCVR git_diff.patch\n",
            "Processing file 1dMZepPGRqW_KZHGX5R1euv034F6KXOsi info.log\n",
            "Processing file 17AT0XyGjTBzMUDdt_on_XmdG3sbVrXLX bpe_model.model\n",
            "Retrieving folder contents completed\n",
            "Building directory structure\n",
            "Building directory structure completed\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1KkuSUvlBGkiulcp8hY8jeMV3FuGOqfkd\n",
            "From (redirected): https://drive.google.com/uc?id=1KkuSUvlBGkiulcp8hY8jeMV3FuGOqfkd&confirm=t&uuid=334370b7-e135-4ddd-811d-a13dc0439a2d\n",
            "To: /content/ASR-main/data/conformer_bpe_M/checkpoint-epoch100.pth\n",
            "100% 297M/297M [00:04<00:00, 66.1MB/s]\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1SFtgFBMI47C1DnSAb2IRyKfoHFH7UhFz\n",
            "From (redirected): https://drive.google.com/uc?id=1SFtgFBMI47C1DnSAb2IRyKfoHFH7UhFz&confirm=t&uuid=6d401114-e100-49c3-b110-5ce76d6d6959\n",
            "To: /content/ASR-main/data/conformer_bpe_M/checkpoint-epoch105.pth\n",
            "100% 297M/297M [00:08<00:00, 35.0MB/s]\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1fWyTqVw82DcGTF6hCireMrocasiZys-Z\n",
            "From (redirected): https://drive.google.com/uc?id=1fWyTqVw82DcGTF6hCireMrocasiZys-Z&confirm=t&uuid=e84a602e-e6c1-4ecf-9d54-c8ebcdd83f0c\n",
            "To: /content/ASR-main/data/conformer_bpe_M/checkpoint-epoch107.pth\n",
            "100% 192M/192M [00:01<00:00, 120MB/s]\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1eODvhRu72YTsXPdFQKoETtGSeaPEVnL8\n",
            "From (redirected): https://drive.google.com/uc?id=1eODvhRu72YTsXPdFQKoETtGSeaPEVnL8&confirm=t&uuid=6f1f5ef9-1335-4892-8ba6-177dad168767\n",
            "To: /content/ASR-main/data/conformer_bpe_M/checkpoint-epoch108.pth\n",
            "100% 95.7M/95.7M [00:02<00:00, 40.1MB/s]\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1OFqS7nn3b22XQ3m433ii12bVNPSDmcpv\n",
            "From (redirected): https://drive.google.com/uc?id=1OFqS7nn3b22XQ3m433ii12bVNPSDmcpv&confirm=t&uuid=0d7e9efb-802e-44e8-ab30-4cde89e4fb01\n",
            "To: /content/ASR-main/data/conformer_bpe_M/checkpoint-epoch110.pth\n",
            "100% 297M/297M [00:06<00:00, 48.9MB/s]\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=19xo98yVcWAN7NsOw4c1lNZJr8HxG1xpU\n",
            "From (redirected): https://drive.google.com/uc?id=19xo98yVcWAN7NsOw4c1lNZJr8HxG1xpU&confirm=t&uuid=52ceb312-a3f3-41df-8839-0a51346f86f9\n",
            "To: /content/ASR-main/data/conformer_bpe_M/checkpoint-epoch115.pth\n",
            "100% 297M/297M [00:08<00:00, 36.4MB/s]\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=17FLeSlod_pteRme-0BhWw5ICQXzD0mKw\n",
            "From (redirected): https://drive.google.com/uc?id=17FLeSlod_pteRme-0BhWw5ICQXzD0mKw&confirm=t&uuid=3df59056-f979-4a7a-be84-acf5188efbc4\n",
            "To: /content/ASR-main/data/conformer_bpe_M/checkpoint-epoch117.pth\n",
            "100% 131M/131M [00:04<00:00, 27.4MB/s]\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1NhAuR9NoPd5fzBtcZYeAFZe8w_pJeUwl\n",
            "From (redirected): https://drive.google.com/uc?id=1NhAuR9NoPd5fzBtcZYeAFZe8w_pJeUwl&confirm=t&uuid=2545703c-1cc2-4ce5-bcc4-1a4e68d285c4\n",
            "To: /content/ASR-main/data/conformer_bpe_M/checkpoint-epoch120.pth\n",
            "100% 297M/297M [00:05<00:00, 57.8MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1Hzara3pbu93e7ugAu-bC2sLNfO_8Ro65\n",
            "To: /content/ASR-main/data/conformer_bpe_M/config.yaml\n",
            "100% 2.77k/2.77k [00:00<00:00, 15.6MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1BYZYuYe-LUO2HGhnd6XmFYiegsZhTjpO\n",
            "To: /content/ASR-main/data/conformer_bpe_M/git_commit.txt\n",
            "100% 41.0/41.0 [00:00<00:00, 241kB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1Zs7nkNN4pQi_IP_AnvhCBC-eZh0gPCVR\n",
            "To: /content/ASR-main/data/conformer_bpe_M/git_diff.patch\n",
            "100% 1.70k/1.70k [00:00<00:00, 9.91MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1dMZepPGRqW_KZHGX5R1euv034F6KXOsi\n",
            "To: /content/ASR-main/data/conformer_bpe_M/info.log\n",
            "100% 134k/134k [00:00<00:00, 1.58MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=17AT0XyGjTBzMUDdt_on_XmdG3sbVrXLX\n",
            "To: /content/ASR-main/data/bpe_model.model\n",
            "100% 251k/251k [00:00<00:00, 2.05MB/s]\n",
            "Download completed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example: Inference on Librispeech"
      ],
      "metadata": {
        "id": "PsxzkT0D9Ruy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To run inference on the LibriSpeech test dataset:\n",
        "\n",
        "```bash\n",
        "python3 inference.py -cn=inference_bpe writer=none text_encoder.model_path=data/bpe_model.model inferencer.from_pretrained=data/best_model/\n",
        "```"
      ],
      "metadata": {
        "id": "Pk_j3XqP9VPq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 inference.py -cn=inference_bpe writer=none text_encoder.model_path=data/bpe_model.model inferencer.from_pretrained=data/best_model/model_best.pth"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2OU3bekz9Ulz",
        "outputId": "d9950848-bbd4-4745-d701-11119dd708ef"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "kenlm python bindings are not installed. Most likely you want to install it using: pip install https://github.com/kpu/kenlm/archive/master.zip\n",
            "kenlm python bindings are not installed. Most likely you want to install it using: pip install https://github.com/kpu/kenlm/archive/master.zip\n",
            "Conformer(\n",
            "  (subsampling): Sequential(\n",
            "    (0): Conv1d(80, 256, kernel_size=(3,), stride=(2,), padding=(1,))\n",
            "    (1): ReLU()\n",
            "    (2): Conv1d(256, 256, kernel_size=(3,), stride=(2,), padding=(1,))\n",
            "    (3): ReLU()\n",
            "  )\n",
            "  (conformer): Conformer(\n",
            "    (conformer_layers): ModuleList(\n",
            "      (0-15): 16 x ConformerLayer(\n",
            "        (ffn1): _FeedForwardModule(\n",
            "          (sequential): Sequential(\n",
            "            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "            (1): Linear(in_features=256, out_features=1024, bias=True)\n",
            "            (2): SiLU()\n",
            "            (3): Dropout(p=0.2, inplace=False)\n",
            "            (4): Linear(in_features=1024, out_features=256, bias=True)\n",
            "            (5): Dropout(p=0.2, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "        (self_attn_dropout): Dropout(p=0.2, inplace=False)\n",
            "        (conv_module): _ConvolutionModule(\n",
            "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (sequential): Sequential(\n",
            "            (0): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
            "            (1): GLU(dim=1)\n",
            "            (2): Conv1d(256, 256, kernel_size=(31,), stride=(1,), padding=(15,), groups=256)\n",
            "            (3): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (4): SiLU()\n",
            "            (5): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
            "            (6): Dropout(p=0.2, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (ffn2): _FeedForwardModule(\n",
            "          (sequential): Sequential(\n",
            "            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "            (1): Linear(in_features=256, out_features=1024, bias=True)\n",
            "            (2): SiLU()\n",
            "            (3): Dropout(p=0.2, inplace=False)\n",
            "            (4): Linear(in_features=1024, out_features=256, bias=True)\n",
            "            (5): Dropout(p=0.2, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (output_projection): Linear(in_features=256, out_features=301, bias=True)\n",
            ")\n",
            "Loading model weights from: data/best_model/model_best.pth ...\n",
            "test: 100% 328/328 [00:51<00:00,  6.40it/s]\n",
            "  CER: 0.10783362539400625\n",
            "  WER: 0.2694591433910096\n",
            "==================================================\n",
            "Inference Results:\n",
            "==================================================\n",
            "\n",
            "TEST partition:\n",
            "    test_CER       : 0.10783362539400625\n",
            "    test_WER       : 0.2694591433910096\n",
            "\n",
            "Inference completed successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Example: Custom Dataset Inference\n"
      ],
      "metadata": {
        "id": "hROXm69wFGfv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Format of the dataset:\n",
        "\n",
        "```arduino\n",
        "\n",
        "\n",
        "NameOfTheDirectoryWithUtterances\n",
        "├── audio\n",
        "│   ├── UtteranceID1.wav # may be flac or mp3\n",
        "│   ├── UtteranceID2.wav\n",
        "│   .\n",
        "│   .\n",
        "│   .\n",
        "│   └── UtteranceIDn.wav\n",
        "└── transcriptions # ground truth, may not exist\n",
        "    ├── UtteranceID1.txt\n",
        "    ├── UtteranceID2.txt\n",
        "    .\n",
        "    .\n",
        "    .\n",
        "    └── UtteranceIDn.txt\n",
        "```\n",
        "\n",
        "Command:\n",
        "\n",
        "```bash\n",
        "python3 inference.py -cn=inference_bpe datasets=custom_dir \\\n",
        "    inferencer.save_dir=inference_results \\\n",
        "    datasets.val.data_dir=test_data/\n",
        "```\n"
      ],
      "metadata": {
        "id": "5yQzaW5gq2YJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 inference.py -cn=inference_bpe datasets=custom_dir inferencer.save_path=inference_results datasets.val.data_dir=test_data text_encoder.model_path=data/bpe_model.model inferencer.from_pretrained=data/best_model/model_best.pth  dataloader.batch_size=1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "oX5cdSrMFIAk",
        "outputId": "2a77e266-09c4-4666-bf9b-5ecb42dca3dd"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "kenlm python bindings are not installed. Most likely you want to install it using: pip install https://github.com/kpu/kenlm/archive/master.zip\n",
            "kenlm python bindings are not installed. Most likely you want to install it using: pip install https://github.com/kpu/kenlm/archive/master.zip\n",
            "/usr/local/lib/python3.12/dist-packages/torch_audiomentations/core/transforms_interface.py:76: FutureWarning: Transforms now expect an `output_type` argument that currently defaults to 'tensor', will default to 'dict' in v0.12, and will be removed in v0.13. Make sure to update your code to something like:\n",
            "  >>> augment = Gain(..., output_type='dict')\n",
            "  >>> augmented_samples = augment(samples).samples\n",
            "  warnings.warn(\n",
            "0 (0.0%) records are longer then 20.0 seconds. Excluding them.\n",
            "0 (0.0%) records are longer then 200 characters. Excluding them.\n",
            "0 (0.0%) records are longer then 20.0 seconds. Excluding them.\n",
            "0 (0.0%) records are longer then 200 characters. Excluding them.\n",
            "Conformer(\n",
            "  (subsampling): Sequential(\n",
            "    (0): Conv1d(80, 256, kernel_size=(3,), stride=(2,), padding=(1,))\n",
            "    (1): ReLU()\n",
            "    (2): Conv1d(256, 256, kernel_size=(3,), stride=(2,), padding=(1,))\n",
            "    (3): ReLU()\n",
            "  )\n",
            "  (conformer): Conformer(\n",
            "    (conformer_layers): ModuleList(\n",
            "      (0-15): 16 x ConformerLayer(\n",
            "        (ffn1): _FeedForwardModule(\n",
            "          (sequential): Sequential(\n",
            "            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "            (1): Linear(in_features=256, out_features=1024, bias=True)\n",
            "            (2): SiLU()\n",
            "            (3): Dropout(p=0.2, inplace=False)\n",
            "            (4): Linear(in_features=1024, out_features=256, bias=True)\n",
            "            (5): Dropout(p=0.2, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "        (self_attn_dropout): Dropout(p=0.2, inplace=False)\n",
            "        (conv_module): _ConvolutionModule(\n",
            "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (sequential): Sequential(\n",
            "            (0): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
            "            (1): GLU(dim=1)\n",
            "            (2): Conv1d(256, 256, kernel_size=(31,), stride=(1,), padding=(15,), groups=256)\n",
            "            (3): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (4): SiLU()\n",
            "            (5): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
            "            (6): Dropout(p=0.2, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (ffn2): _FeedForwardModule(\n",
            "          (sequential): Sequential(\n",
            "            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "            (1): Linear(in_features=256, out_features=1024, bias=True)\n",
            "            (2): SiLU()\n",
            "            (3): Dropout(p=0.2, inplace=False)\n",
            "            (4): Linear(in_features=1024, out_features=256, bias=True)\n",
            "            (5): Dropout(p=0.2, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (output_projection): Linear(in_features=256, out_features=301, bias=True)\n",
            ")\n",
            "Error executing job with overrides: ['datasets=custom_dir', 'inferencer.save_path=inference_results', 'datasets.val.data_dir=test_data', 'text_encoder.model_path=data/bpe_model.model', 'inferencer.from_pretrained=data/best_model/model_best.pth', 'dataloader.batch_size=1']\n",
            "Error in call to target 'src.logger.cometml.CometMLWriter':\n",
            "TypeError(\"CometMLWriter.__init__() missing 2 required positional arguments: 'logger' and 'project_config'\")\n",
            "full_key: writer\n",
            "\n",
            "Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you wish to use a link for the dataset, download your dataset first:"
      ],
      "metadata": {
        "id": "lj2eLgK5sV45"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#If you want do download Google Drive dataset:\n",
        "\n",
        "!chmod +x download_gdrive_datasets.sh\n",
        "!./download_gdrive_datasets.sh DIR_NAME YOUR_LINK\n",
        "\n",
        "\n",
        "#If you can easily access  it, just use wget:\n",
        "!wget YOUR_LINK"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8lBRY8lQsjjw",
        "outputId": "1d745ea6-5674-4685-bc07-57d5df3c6308"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from Google Drive...\n",
            "Retrieving folder contents\n",
            "Failed to retrieve folder contents\n",
            "Download completed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 inference.py -cn=inference_bpe  writer=none datasets=custom_dir inferencer.save_path=inference_results datasets.val.data_dir=custom_data text_encoder.model_path=data/bpe_model.model inferencer.from_pretrained=data/best_model/model_best.pth  dataloader.batch_size=1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "wbjE-OuTutXX",
        "outputId": "e544af2c-51fb-4d22-bb7f-05a7c9295ca1"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "kenlm python bindings are not installed. Most likely you want to install it using: pip install https://github.com/kpu/kenlm/archive/master.zip\n",
            "kenlm python bindings are not installed. Most likely you want to install it using: pip install https://github.com/kpu/kenlm/archive/master.zip\n",
            "/usr/local/lib/python3.12/dist-packages/torch_audiomentations/core/transforms_interface.py:76: FutureWarning: Transforms now expect an `output_type` argument that currently defaults to 'tensor', will default to 'dict' in v0.12, and will be removed in v0.13. Make sure to update your code to something like:\n",
            "  >>> augment = Gain(..., output_type='dict')\n",
            "  >>> augmented_samples = augment(samples).samples\n",
            "  warnings.warn(\n",
            "0 (0.0%) records are longer then 20.0 seconds. Excluding them.\n",
            "0 (0.0%) records are longer then 200 characters. Excluding them.\n",
            "0 (0.0%) records are longer then 20.0 seconds. Excluding them.\n",
            "0 (0.0%) records are longer then 200 characters. Excluding them.\n",
            "Conformer(\n",
            "  (subsampling): Sequential(\n",
            "    (0): Conv1d(80, 256, kernel_size=(3,), stride=(2,), padding=(1,))\n",
            "    (1): ReLU()\n",
            "    (2): Conv1d(256, 256, kernel_size=(3,), stride=(2,), padding=(1,))\n",
            "    (3): ReLU()\n",
            "  )\n",
            "  (conformer): Conformer(\n",
            "    (conformer_layers): ModuleList(\n",
            "      (0-15): 16 x ConformerLayer(\n",
            "        (ffn1): _FeedForwardModule(\n",
            "          (sequential): Sequential(\n",
            "            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "            (1): Linear(in_features=256, out_features=1024, bias=True)\n",
            "            (2): SiLU()\n",
            "            (3): Dropout(p=0.2, inplace=False)\n",
            "            (4): Linear(in_features=1024, out_features=256, bias=True)\n",
            "            (5): Dropout(p=0.2, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "        (self_attn_dropout): Dropout(p=0.2, inplace=False)\n",
            "        (conv_module): _ConvolutionModule(\n",
            "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (sequential): Sequential(\n",
            "            (0): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
            "            (1): GLU(dim=1)\n",
            "            (2): Conv1d(256, 256, kernel_size=(31,), stride=(1,), padding=(15,), groups=256)\n",
            "            (3): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (4): SiLU()\n",
            "            (5): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
            "            (6): Dropout(p=0.2, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (ffn2): _FeedForwardModule(\n",
            "          (sequential): Sequential(\n",
            "            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "            (1): Linear(in_features=256, out_features=1024, bias=True)\n",
            "            (2): SiLU()\n",
            "            (3): Dropout(p=0.2, inplace=False)\n",
            "            (4): Linear(in_features=1024, out_features=256, bias=True)\n",
            "            (5): Dropout(p=0.2, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (output_projection): Linear(in_features=256, out_features=301, bias=True)\n",
            ")\n",
            "Writer initialized for logging inference results\n",
            "Loading model weights from: data/best_model/model_best.pth ...\n",
            "train: 100% 5/5 [00:00<00:00,  5.90it/s]\n",
            "Error executing job with overrides: ['writer=none', 'datasets=custom_dir', 'inferencer.save_path=inference_results', 'datasets.val.data_dir=custom_data', 'text_encoder.model_path=data/bpe_model.model', 'inferencer.from_pretrained=data/best_model/model_best.pth', 'dataloader.batch_size=1']\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/ASR-main/inference.py\", line 96, in main\n",
            "    logs = inferencer.run_inference()\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/ASR-main/src/trainer/inferencer.py\", line 106, in run_inference\n",
            "    logs = self._inference_part(part, dataloader)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/ASR-main/src/trainer/inferencer.py\", line 384, in _inference_part\n",
            "    self.writer.add_scalars(results)\n",
            "    ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "omegaconf.errors.ConfigAttributeError: Missing key add_scalars\n",
            "    full_key: writer.add_scalars\n",
            "    object_type=dict\n",
            "\n",
            "Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Example: Separate Metrics Calculation\n",
        "\n",
        "If you already have ground truth and prediction files, you can evaluate them directly with the following metrics script:\n",
        "\n",
        "```bash\n",
        "python scripts/calc_metrics.py --pred_dir=your/predictions/dir --target_dir=your/ground_truth/dir\n",
        "```"
      ],
      "metadata": {
        "id": "7sP8prZiqgxT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python scripts/calc_metrics.py --pred_dir=data/saved/inference_results/val --target_dir=test_data/transcriptions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "leKQEGkNicD9",
        "outputId": "13593357-3116-4cc0-e8a9-cee10618029a"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "kenlm python bindings are not installed. Most likely you want to install it using: pip install https://github.com/kpu/kenlm/archive/master.zip\n",
            "kenlm python bindings are not installed. Most likely you want to install it using: pip install https://github.com/kpu/kenlm/archive/master.zip\n",
            "WER: 0.1744202896952629\n",
            "CER: 0.06649788096547127\n"
          ]
        }
      ]
    }
  ]
}
