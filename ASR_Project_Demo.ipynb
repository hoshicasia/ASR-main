{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Installation"
      ],
      "metadata": {
        "id": "q_Ao4QAZ22cn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/hoshicasia/ASR-main.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-wkO_IcO1ydc",
        "outputId": "93794581-1594-4e75-da96-b6a85ae8bbe7"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ASR-main'...\n",
            "remote: Enumerating objects: 450, done.\u001b[K\n",
            "remote: Counting objects: 100% (450/450), done.\u001b[K\n",
            "remote: Compressing objects: 100% (301/301), done.\u001b[K\n",
            "remote: Total 450 (delta 216), reused 364 (delta 130), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (450/450), 835.62 KiB | 12.29 MiB/s, done.\n",
            "Resolving deltas: 100% (216/216), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd ASR-main"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5mbxburn1nzF",
        "outputId": "cc4b29ca-447d-4651-d219-8a20cb59c0a0"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/ASR-main\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -r requirements.txt"
      ],
      "metadata": {
        "collapsed": true,
        "id": "GwxV3QSK1t2G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!chmod +x download_checkpoints.sh"
      ],
      "metadata": {
        "id": "5OhwhxBI5Vgd"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./download_checkpoints.sh"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Nw1FpO3_480F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example: Inference on Librispeech"
      ],
      "metadata": {
        "id": "PsxzkT0D9Ruy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To run inference on the LibriSpeech test dataset:\n",
        "\n",
        "```bash\n",
        "python3 inference.py -cn=inference_bpe writer=none text_encoder.model_path=data/bpe_model.model inferencer.from_pretrained=data/best_model/\n",
        "```"
      ],
      "metadata": {
        "id": "Pk_j3XqP9VPq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 inference.py -cn=inference_bpe writer=none text_encoder.model_path=data/bpe_model.model inferencer.from_pretrained=data/best_model/model_best.pth"
      ],
      "metadata": {
        "id": "2OU3bekz9Ulz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Example: Custom Dataset Inference\n"
      ],
      "metadata": {
        "id": "hROXm69wFGfv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Format of the dataset:\n",
        "\n",
        "```arduino\n",
        "\n",
        "\n",
        "NameOfTheDirectoryWithUtterances\n",
        "├── audio\n",
        "│   ├── UtteranceID1.wav # may be flac or mp3\n",
        "│   ├── UtteranceID2.wav\n",
        "│   .\n",
        "│   .\n",
        "│   .\n",
        "│   └── UtteranceIDn.wav\n",
        "└── transcriptions # ground truth, may not exist\n",
        "    ├── UtteranceID1.txt\n",
        "    ├── UtteranceID2.txt\n",
        "    .\n",
        "    .\n",
        "    .\n",
        "    └── UtteranceIDn.txt\n",
        "```\n",
        "\n",
        "Command:\n",
        "\n",
        "```bash\n",
        "python3 inference.py -cn=inference_bpe datasets=custom_dir \\\n",
        "    inferencer.save_dir=inference_results \\\n",
        "    datasets.val.data_dir=test_data/\n",
        "```\n"
      ],
      "metadata": {
        "id": "5yQzaW5gq2YJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 inference.py -cn=inference_bpe writer=none datasets=custom_dir inferencer.save_path=inference_results datasets.val.data_dir=test_data text_encoder.model_path=data/bpe_model.model inferencer.from_pretrained=data/best_model/model_best.pth  dataloader.batch_size=1"
      ],
      "metadata": {
        "id": "oX5cdSrMFIAk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you wish to use a link for the dataset, download your dataset first:"
      ],
      "metadata": {
        "id": "lj2eLgK5sV45"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#If you want do download Google Drive dataset:\n",
        "\n",
        "!chmod +x download_gdrive_datasets.sh\n",
        "!./download_gdrive_datasets.sh YOUR_DIR YOUR_LINK\n",
        "\n",
        "\n",
        "#If you can easily access  it, just use wget:\n",
        "!wget YOUR_LINK"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8lBRY8lQsjjw",
        "outputId": "1d745ea6-5674-4685-bc07-57d5df3c6308"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from Google Drive...\n",
            "Retrieving folder contents\n",
            "Failed to retrieve folder contents\n",
            "Download completed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 inference.py -cn=inference_bpe  datasets.val.data_dir=YOUR_DIR writer=none datasets=custom_dir inferencer.save_path=inference_results text_encoder.model_path=data/bpe_model.model inferencer.from_pretrained=data/best_model/model_best.pth  dataloader.batch_size=1 inferencer.decode_method=\"argmax\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "wbjE-OuTutXX",
        "outputId": "c0ea8dcc-74e8-4e0b-c0fd-27a2096de89a"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "kenlm python bindings are not installed. Most likely you want to install it using: pip install https://github.com/kpu/kenlm/archive/master.zip\n",
            "kenlm python bindings are not installed. Most likely you want to install it using: pip install https://github.com/kpu/kenlm/archive/master.zip\n",
            "/usr/local/lib/python3.12/dist-packages/torch_audiomentations/core/transforms_interface.py:76: FutureWarning: Transforms now expect an `output_type` argument that currently defaults to 'tensor', will default to 'dict' in v0.12, and will be removed in v0.13. Make sure to update your code to something like:\n",
            "  >>> augment = Gain(..., output_type='dict')\n",
            "  >>> augmented_samples = augment(samples).samples\n",
            "  warnings.warn(\n",
            "0 (0.0%) records are longer then 20.0 seconds. Excluding them.\n",
            "0 (0.0%) records are longer then 200 characters. Excluding them.\n",
            "0 (0.0%) records are longer then 20.0 seconds. Excluding them.\n",
            "0 (0.0%) records are longer then 200 characters. Excluding them.\n",
            "Conformer(\n",
            "  (subsampling): Sequential(\n",
            "    (0): Conv1d(80, 256, kernel_size=(3,), stride=(2,), padding=(1,))\n",
            "    (1): ReLU()\n",
            "    (2): Conv1d(256, 256, kernel_size=(3,), stride=(2,), padding=(1,))\n",
            "    (3): ReLU()\n",
            "  )\n",
            "  (conformer): Conformer(\n",
            "    (conformer_layers): ModuleList(\n",
            "      (0-15): 16 x ConformerLayer(\n",
            "        (ffn1): _FeedForwardModule(\n",
            "          (sequential): Sequential(\n",
            "            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "            (1): Linear(in_features=256, out_features=1024, bias=True)\n",
            "            (2): SiLU()\n",
            "            (3): Dropout(p=0.2, inplace=False)\n",
            "            (4): Linear(in_features=1024, out_features=256, bias=True)\n",
            "            (5): Dropout(p=0.2, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "        (self_attn_dropout): Dropout(p=0.2, inplace=False)\n",
            "        (conv_module): _ConvolutionModule(\n",
            "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (sequential): Sequential(\n",
            "            (0): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
            "            (1): GLU(dim=1)\n",
            "            (2): Conv1d(256, 256, kernel_size=(31,), stride=(1,), padding=(15,), groups=256)\n",
            "            (3): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (4): SiLU()\n",
            "            (5): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
            "            (6): Dropout(p=0.2, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (ffn2): _FeedForwardModule(\n",
            "          (sequential): Sequential(\n",
            "            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "            (1): Linear(in_features=256, out_features=1024, bias=True)\n",
            "            (2): SiLU()\n",
            "            (3): Dropout(p=0.2, inplace=False)\n",
            "            (4): Linear(in_features=1024, out_features=256, bias=True)\n",
            "            (5): Dropout(p=0.2, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (output_projection): Linear(in_features=256, out_features=301, bias=True)\n",
            ")\n",
            "Loading model weights from: data/best_model/model_best.pth ...\n",
            "train: 100% 5/5 [00:02<00:00,  2.48it/s]\n",
            "  CER: 0.039065960235893725\n",
            "  WER: 0.10839130654931069\n",
            "Logged 5 predictions to experiment tracker\n",
            "val: 100% 5/5 [00:01<00:00,  3.10it/s]\n",
            "  CER: 0.039065960235893725\n",
            "  WER: 0.10839130654931069\n",
            "Logged 5 predictions to experiment tracker\n",
            "==================================================\n",
            "Inference Results:\n",
            "==================================================\n",
            "\n",
            "TRAIN partition:\n",
            "    train_CER      : 0.039065960235893725\n",
            "    train_WER      : 0.10839130654931069\n",
            "\n",
            "VAL partition:\n",
            "    val_CER        : 0.039065960235893725\n",
            "    val_WER        : 0.10839130654931069\n",
            "\n",
            "Inference completed successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Example: Separate Metrics Calculation\n",
        "\n",
        "If you already have ground truth and prediction files, you can evaluate them directly with the following metrics script:\n",
        "\n",
        "```bash\n",
        "python scripts/calc_metrics.py --pred_dir=your/predictions/dir --target_dir=your/ground_truth/dir\n",
        "```"
      ],
      "metadata": {
        "id": "7sP8prZiqgxT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python scripts/calc_metrics.py --pred_dir=data/saved/inference_results/val --target_dir=test_data/transcriptions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "leKQEGkNicD9",
        "outputId": "42ac576b-f9c2-4952-80dd-9adefad3bcb5"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "kenlm python bindings are not installed. Most likely you want to install it using: pip install https://github.com/kpu/kenlm/archive/master.zip\n",
            "kenlm python bindings are not installed. Most likely you want to install it using: pip install https://github.com/kpu/kenlm/archive/master.zip\n",
            "WER: 0.10839130654931069\n",
            "CER: 0.039065960235893725\n"
          ]
        }
      ]
    }
  ]
}
